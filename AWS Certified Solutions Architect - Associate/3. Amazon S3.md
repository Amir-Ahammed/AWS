# Amazon S3 (Simple Storage Service)
Amazon S3 (Simple Storage Service) is a **highly scalable, durable, and secure object storage service** offered by AWS. It‚Äôs designed to store and retrieve **any amount of data** from **anywhere on the web**, making it a backbone for cloud-native applications, data lakes, backups, and more

## Amazon S3: Common Uses
1. **Backup and Storage** - Serves as a central repository for data backups.
2. **Disaster Recovery** - Enables quick recovery of critical data in case of failure.
3. **Archiving** - Offers low-cost storage classes (e.g., Glacier) ideal for infrequently accessed data.
4. **Hybrid Cloud Storage** - Acts as an extension of on-premises infrastructure.
5. **Application Hosting** - Stores static assets (e.g., HTML, CSS, JavaScript) for web or mobile apps. Also enables integration with AWS Lambda or EC2 for dynamic applications.
6. **Media Hosting** - Hosts videos, audio, and images at scale.
7. **Data Lakes and Big Data Analytics** - Central storage for structured and unstructured data.
8. **Software Delivery** - Distributes software binaries, patches, updates, and installers globally.
9. **Static Website Hosting** - Allows fully hosted, serverless websites.

> üìå *Amazon S3 is foundational to many AWS services and architectures‚Äîmastering it sets the stage for cloud-native development.*

# Amazon S3: Features

## 1. Storage Classes
- **S3 Standard**: General-purpose, frequent access.
- **S3 Intelligent-Tiering**: Automated cost optimization.
- **S3 Standard-IA / One Zone-IA**: Infrequent access options.
- **S3 Glacier / Glacier Deep Archive**: Archival storage.
- **S3 Express One Zone**: Ultra-low latency, single AZ.
- **S3 Outposts** ‚Äì On-premises S3 storage.
<details>
  <summary>Storage Classes with Pricing (Mumbai Region)</summary>

## Storage Classes with Pricing (Mumbai Region)

| Storage Class             | Retrieval Time     | Min. Duration | Use Case                          | Approx. Storage Price/GB*        |
|---------------------------|--------------------|---------------|-----------------------------------|----------------------------------|
| S3 Standard               | Milliseconds       | None          | Active content, high demand       | $0.023                           |
| S3 Intelligent-Tiering    | Millisec to Hours  | 30‚Äì180 days   | Unpredictable access patterns     | $0.023 (frequent), + monitoring fee |
| S3 Standard-IA            | Milliseconds       | 30 days       | Backups, infrequent access        | $0.016                           |
| S3 One Zone-IA            | Milliseconds       | 30 days       | Low-priority, single-AZ storage   | $0.0128                          |
| S3 Glacier Instant        | Milliseconds       | 90 days       | Archives with instant access      | $0.0115                          |
| S3 Glacier Flexible       | 1 min ‚Äì 12 hours   | 90 days       | Long-term archives, flexible needs| $0.0046                          |
| S3 Glacier Deep Archive   | 12 ‚Äì 48 hours      | 180 days      | Long-term, lowest-cost storage    | $0.00099                         |
| S3 Outposts               | Milliseconds       | 1 month       | Data residency/compliance         | $0.0235 - $0.0375  per GB/month  |
</details>

## 2. Data Management & Organization
- **Buckets & Objects** ‚Äì Store files (objects) in buckets (containers).
- **Versioning** ‚Äì Keep multiple versions of an object.
- **Lifecycle Policies**: Automate transitions between storage classes.
- **Object Lock (WORM)**: Prevent object deletion or modification for a set time to meet compliance (WORM).
- **S3 Replication**: Automatically copy objects between buckets (same or different regions) for backup, compliance, or low-latency access.
- **S3 Batch Operations**: Perform actions (like copy, tag, restore, or invoke Lambda) on billions of objects in bulk with a single job.
<details>
  <summary>Explore More‚ÄîHelmet On!ü§Ø</summary>

### ‚ôªÔ∏è S3/Lifecycle
An S3 Lifecycle Policy is a rule that automates what happens to your objects over time‚Äîlike moving them to cheaper storage or deleting them when they're no longer needed.
<details>
<summary>Why use it?</summary>

### Why use it?**
- To **save money** by moving infrequently accessed data to lower-cost storage classes.
- To **automate cleanup** by deleting old or temporary files you no longer need.
- To maintain **data compliance**, ensuring files are retained or deleted on a schedule that meets legal or business policies.
- To manage **object versioning**, automatically removing old versions that accumulate over time.
- To clean up **incomplete multipart uploads** and avoid being charged for unfinished file transfers.
- To support an **automated archival strategy**, like sending inactive documents or media files to Glacier without writing custom code.
</details>
<details>
<summary>How It Works</summary>

### How It Works**
- You can **define** rules inside a bucket.
- Each rule can target **specific objects** (using prefixes or tags).
- You set **conditions** like:
  - "After X days, move object to cheaper storage,‚Äù or ‚ÄúDelete object after Y days.‚Äù (Ex: After 30 days ‚Üí move to S3 Glacier, After 365 days ‚Üí delete)
  - AWS watches the object age and applies the action automatically.

> üìå Tip: Lifecycle rules apply to both **existing** and **new** objects that match the rule criteria.
</details>

### üîí S3/Object Lock
S3 Object Lock is a feature that lets you **protect objects from being deleted or overwritten** for a fixed time or indefinitely, using a **WORM (Write Once, Read Many)** model.
<details>
<summary>Why to use?</summary>

### Why use it?
- To meet **compliance requirements** (e.g., SEC, FINRA) that require data immutability.
- To protect critical data from **accidental or malicious deletion** (e.g., ransomware).
- To enforce **retention policies** for legal or business needs.
</details>
<details>
  <summary>How It Work</summary>

### How It Work
1. **Enable versioning** on the bucket (required).
2. Turn on **Object Lock** when creating the bucket.
3. Apply either:
   - A **Retention Period**: Locks the object for a set time.
   - A **Legal Hold**: Locks the object until manually removed.
4. During the lock, the object **cannot be modified or deleted**, even by admins.

> üìå Tip: Use S3 Object Lock with versioning to safeguard critical data from accidental or malicious deletion‚Äîeven administrators can‚Äôt bypass it during the retention period.
</details>


### üì¶ S3/Batch Operations
S3 Batch Operations is a feature that lets you **perform actions on billions of S3 objects at once**‚Äîlike copying, tagging, restoring, or invoking Lambda functions‚Äîusing a single job.
<details>
<summary>Why use it?</summary>

### Why use it?**
- To **automate repetitive tasks** across large datasets.
- To **save time and effort** compared to scripting or manual work.
- To **update metadata**, restore archived data, or apply access controls in bulk.
- To **trigger custom processing** (e.g., resizing images) using AWS Lambda.
</details>
<details>
<summary>How it work</summary>

### How it work
1. Create a **manifest** (list of target objects) using S3 Inventory or a CSV file.
2. Define a **job** with the desired action (e.g., copy, tag, restore).
3. Submit the job via the S3 Console, CLI, SDK, or API.
4. AWS processes the job, tracks progress, retries failures, and generates a **completion report**.
</details>
</details>

## 3. Security & Access Control
Amazon S3 offers a combination of ***user-based*** and ***resource-based*** security mechanisms to control access to ***buckets*** and ***objects***. Security in S3 is managed primarily through ***IAM***, ***Bucket policies***, ***Access Control Lists,*** and ***Encryption***.

### 1. User-Based Access Control
* **IAM Policies**
  - Define which **API actions** a specific IAM **user, group, or role** can perform.
  - Managed centrally in **IAM**, not in the S3 service directly.
  - Can allow or deny operations like `s3:GetObject`, `s3:PutObject`, etc.

***Example: User Access to S3 ‚Äî Using IAM Permission*** <br> **Use Case:** Grant specific IAM users or groups access to S3 buckets using IAM policies.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::examplebucket",
        "arn:aws:s3:::examplebucket/*"
      ]
    }
  ]
}
```
> üìå IAM policies are user-based and should follow the principle of least privilege.

***Example: EC2 Instance Access ‚Äî Using IAM Role*** <br> **Use Case:** Allow an EC2 instance to access an S3 bucket (e.g., for data processing or logging).
**Steps:**
- Create an **IAM Role** with an attached policy for S3 access.
- Attach the role to your EC2 instance.
- The EC2 instance uses **temporary credentials** to access S3.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```
> üìå Ideal for avoiding long-term access keys in instance profiles.

### 2. Resource-Based Access Control
* **Bucket Policy (most common)** 
  - JSON-based policy attached to a bucket.
  - Can grant **cross-account access**.
  - Useful for **public access settings**, **service permissions**, and **fine-grained control** over bucket access.
* **Object Access Control List (ACL)**
  - Legacy method for granting **individual permissions** to specific users for individual objects.
  - Can be disabled for security best practice.
* **Bucket Access Control List (ACL)**
  - Legacy method for setting permissions at the bucket level.
  - Rarely used today; can also be disabled.

### Bucket Policies
A **bucket policy** is a JSON-based resource policy attached directly to an S3 bucket. It defines **who can access** the bucket or its objects, **which actions** are allowed or denied, and **what conditions** apply to those permissions.

### Key Elements of a Bucket Policy
- **Effect**: Allow or Deny.
- **Principal**: The user, role, or account the policy applies to.
- **Action**: Specific S3 API actions (e.g., `s3:GetObject`, `s3:PutObject`).
- **Resource**: Amazon Resource Name (ARN)l of the bucket or objects (e.g., `arn:aws:s3:::examplebucket/*`).
- **Condition** (optional): Constraints like IP address, secure transport, or MFA.

***Example: Public Access ‚Äî Using Bucket Policy*** <br> **Use Case:** Allow anyone on the internet to read objects in a bucket (common for hosting static websites or public datasets).

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```
> üìå Important: Always check and block public access settings intentionally if making a bucket public.

***Example: Cross-Account Access ‚Äî Using Bucket Policy*** <br> **Use Case:** Allow another AWS account to access your S3 bucket.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "CrossAccountAccess",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:root"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```
> üìå This setup allows users from Account B to access resources in Account A's S3 bucket.

### Encryption
When you store data in S3, Amazon can encrypt it to keep it secure
- S3 supports two types of Protection/Encryption:
  - **at rest:** When data is stored (saved).
  - **in transit:** When data is being sent over the internet.
- Encryption Options:
  - **SSE-S3 (Server-Side Encryption with S3-managed keys)**: Server-side encryption with Amazon-managed keys.
  - **SSE-KMS (Server-Side Encryption with AWS Key Management Service)**: Encryption with AWS KMS-managed keys.
  - **SSE-C (Server-Side Encryption with Customer-provided keys)**: Customer-provided keys.
- Clients can also encrypt data **before uploading** to S3.

### Block Public Access
AWS provides settings to block public access to S3 buckets at the account or bucket level. These settings help prevent accidental exposure of sensitive data.

### Configuration Table

| Setting                                                                 | Default | Description                                                                 |
|-------------------------------------------------------------------------|---------|-----------------------------------------------------------------------------|
| Block all public access                                                 | On      | Disables all public access regardless of ACLs or policies                   |
| Block public access through new ACLs                                    | On      | Prevents public access granted via newly created ACLs                       |
| Block public access through any ACLs                                    | On      | Denies all public access granted via any ACLs                               |
| Block public access through new public bucket/access point policies     | On      | Blocks new attempts to make the bucket public using policies                |
| Block public and cross-account access via any bucket/access point policies | On   | Disallows both public and cross-account access unless explicitly permitted  |

> üìå These settings can be configured at the **account level** or **per bucket**.  
> üìå Recommended for most use cases where buckets are not intended to be public.

> ### Authorization Logic
> An IAM principal (user or role) can access an S3 object **if:** The **IAM policy allows** it **OR** the **resource policy (e.g., bucket policy)** allows it **AND** There is **no explicit deny** in any applicable policy.

## Data Transfer & Replication
### üåÄ S3/Replication
S3 Replication automatically copies objects from one bucket to another‚Äîeither in the same AWS Region or across different Regions.

<details>
<summary>Why to use?</summary>

### Why to use?
- **Disaster recovery**: Keep backups in separate locations.
- **Compliance**: Meet data residency or redundancy requirements.
- **Low-latency access**: Serve users faster by storing data closer to them.
- **Cross-account backup**: Replicate to buckets in other AWS accounts.
</details>
<details>
  <summary>Types of Replication</summary>
  
### Replication Types:
### 1. Live Replication
Live Replication automatically copies *new* or *updated* objects to another bucket **as they are uploaded** to the source bucket.
- It does **not** replicate objects that already existed before replication was set up.
- To replicate existing objects, use **On-Demand Replication**.
<details>
  <summary>Types of Live Replication</summary>

### Types of `Live Replication`:
- **Cross-Region Replication (CRR):** Replicates to a bucket in a *different AWS Region*.
- **Same-Region Replication (SRR):** Replicates to a bucket in the *same AWS Region*.
</details>
<details>
<summary>How Live Replication (SRR or CRR) Works</summary>

### How Live Replication (SRR or CRR) Works
### Prerequisites
- You must have a Source & Destination bucket available. 
- Versioning must be enabled on **both source and destination buckets**
- Source and destination buckets can be in:
  - **Same Region** ‚Üí Same-Region Replication (SRR)
  - **Different Regions** ‚Üí Cross-Region Replication (CRR)
- An **IAM role** must allow S3 to replicate on your behalf
- No conflicting replication rules in the source bucket
- Destination bucket must have proper **permissions & ownership**
- Optional: configure encryption, tags, or storage class beforehand

### How It Works
Create a **Replication Rule** In the source bucket:
1. Go to **Management ‚Üí Replication rules** ‚Üí Create rule
2. Rule name: Ex: `replicate-to-backup`
3. Scope: Apply to all objects in the bucket
4. Choose **destination bucket**
5. Use or create an **IAM role** (S3 can do this for you)
6. Optional: (tick boxes if you want to replicate)
   - Tags
   - Encryption
   - Additional replication options (eg: RTC, Delete marker replication )

### Note
- Replication is **one-way**
- You must set up **separate rules** for bidirectional replication
- Replication does **not apply to existing objects** (use Batch Replication for that)
</details>

### 2. On-Demand Replication (S3 Batch Replication)
Used to manually replicate *existing* objects that were already in the source bucket before replication was enabled.
- Supports replication to one or more destination buckets.
- Useful for syncing historical data.

</details>
<details>
  <summary>Additional Replication Settings</summary>

### Additional Replication Settings:
- **Multi-Destination Replication:** Replicates objects from one source bucket to *multiple* destination buckets.
- **Replication Time Control (RTC):** Ensures **99.99%** of new objects are replicated within **15 minutes**. Includes metrics.
- **Bi-Directional Replication:** Automatically syncs changes between *both* buckets, keeping them in sync.
- **Replication Metrics:** Monitors:
  - Pending object count and size
  - Maximum replication time
  - Replication failures
- **Replica Modification Sync:** Syncs metadata changes made to replicas (from destination bucket) back to the source.
- **Delete Marker Replication:**
  - **Manual deletes** replicate delete markers.
  - **Lifecycle rule deletes** do **not** replicate delete markers.
</details>

<details>
  <summary>How does it work?</summary>

***Example Replication Rule (JSON)***
```json
{
  "Role": "arn:aws:iam::123456789012:role/s3-replication-role",
  "Rules": [
    {
      "ID": "replicate-logs",
      "Status": "Enabled",
      "Prefix": "logs/",
      "Destination": {
        "Bucket": "arn:aws:s3:::my-destination-bucket",
        "StorageClass": "STANDARD_IA"
      }
    }
  ]
}
```
</details>

<details>
  <summary>Additional Note</summary>



### Additional Note:
* **Delete Behavior**
  - **Delete markers** can be replicated to the destination bucket (optional).
  - **Versioned deletions** (i.e., deletes with a version ID) are **not replicated** to avoid malicious or accidental permanent deletions.
* **Replication Chaining**
  - Replication **is not transitive**:
    - If Bucket A replicates to Bucket B, and B replicates to C ‚Äî objects uploaded to A **will not reach C**.
    - Replication policies only apply *from the direct source bucket*.

> üìå *Amazon S3 Replication increases durability, availability, and compliance flexibility by duplicating objects across buckets and regions.*
</details>
### S3 Storage Lens
### AWS DataSync

## 4. Data Processing & Query
- **S3 Select**: SQL-like query on object content.
- **Amazon Athena**: Serverless query for S3 data.
- **S3 Object Lambda**: Customize object retrieval dynamically.

## 5. Monitoring & Analytics
- **S3 Storage Lens**: Usage metrics across accounts.
- **Access Logs**: Track request activity.
- **CloudWatch Integration**: Operational monitoring.

## 7. Scalability & Durability
- **Unlimited Scalability:** Handles petabytes of data seamlessly.
- **High Durability:** 99.999999999% (11 9's) durability.
- **Availability:** Designed for 99.99% uptime.

# How Amazon S3 works
Amazon S3 is an object storage service that stores data as objects, hierarchical data, or tabular data within buckets. An object is a file and any metadata that describes the file. A bucket is a container for objects.

### Core Concepts
| Term            | Description                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| **Bucket**      | Top-level container for objects (like a folder).                           |
| **Object**      | Data stored in S3, consisting of data, metadata, and a unique key.          |
| **Key**         | Unique identifier for an object within a bucket.                           |
| **Region**      | Physical location where your bucket/data resides.                          |

## Amazon S3: Buckets 
An Amazon S3 **bucket** is a container for storing **objects** (data files). All objects are stored in buckets, making them a foundational concept in S3.
- Each object is stored in a specific **bucket**.
- Buckets must have a **globally unique name (across all regions all accounts)**
- Buckets are **created in a specific AWS region** but can be accessed globally (S3 *appears* global).
- Proper bucket naming is critical for DNS compatibility and service integration.

### Buckets: Naming Rules
- Bucket names must be **globally unique** across all AWS accounts and regions.
- Name requirements:
  - Must be **3‚Äì63 characters** in length.
  - No uppercase letters or underscores.
  - Must start with a **lowercase letter or number**.
  - Cannot be formatted as an IP address (e.g., `192.168.1.1`).
  - Must **not** start with the prefix `xn--`.
  - Must **not** end with the suffix `-s3alias`.

## Amazon S3: Object
In Amazon S3, an **object** is the fundamental unit of storage. Each object contains:
- **Data**: The actual content (e.g., file, image, JSON).
- **Metadata**: Information about the object in key:value form (either system-defined or user-defined).
  - When you upload an image to Amazon S3, metadata provides additional information about the object.
  - System-defined Metadata:
    - `Content-Type: image/jpeg` ‚Äì Tells the browser it's a JPEG image.
  - User-defined Metadata:
    - `x-amz-meta-title: Beach Sunset` ‚Äì Custom title set by the uploader.
    - `x-amz-meta-uploaded-by: Amir` ‚Äì Indicates who uploaded the image.
- **Key**: The unique identifier for the object within a bucket.
  - The `key` is like the full path or filename in the bucket.
  - S3 doesn‚Äôt have real folders‚Äîthe ‚Äúfolder structure‚Äù is simulated by key naming using slashes (`/`).
  - Example of Key: `documents/reports/2024/q1-summary.pdf`
    - Prefix: `documents/reports/2024/`
    - Object Name: `q1-summary.pdf`
- **Version ID**: A unique identifier for the version of the object (if versioning is enabled).
  - When versioning is enabled on a bucket, each object version gets a unique ID.
  - Example: `VersionId: 3HL4kqtJlcpXrof3fjRbJ6G2f8mFvbzF`
- **Tags**: Up to 10 Unicode key:value pairs assigned to help with lifecycle management, security, or classification.
  - Tags in Amazon S3 are key:value pairs that help you organize and manage your objects.
  - Example Tags for an Image File:
    - `Project: Marketing2025`
    - `Owner: Amir`
    - `Environment: Production`

### Object Values
- Objects larger than **5GB** require a **multi-part upload** process.
- Maximum object size: **5TB (5000GB)**

### Versioning
Amazon S3 **versioning** allows you to preserve, retrieve, and restore every version of every object stored in a bucket. It helps protect against accidental overwrites and deletions.

### Key Concepts
- **Enabled at the bucket level**.
- When versioning is enabled:
  - Uploading a file with the **same key** creates a **new version** (e.g., version 1, 2, 3...).
  - You can retrieve or restore previous versions if needed.

> **Best Practice**: Enable versioning to protect important data & revert to earlier versions easily.

### Notes
- Files that existed **before versioning** was enabled are assigned a special version: `"null"`.
- **Suspending versioning** does not delete existing versions‚Äîit only stops new versions from being created.

### Visual Example

A file stored in:  
`s3://my-bucket/my-file.docx`

May have:
- Version 1  
- Version 2  
- Version 3  
...and so on.

Each version is uniquely identified and retrievable.

> üìå *Versioning adds resilience and traceability to your data, making it an essential feature for secure and recoverable storage.*

> ## Best Practices
> - Use **IAM roles** for applications instead of long-term access keys.
> - **Block public access** at the account and bucket level unless explicitly needed.
> - Enforce **encryption** for both data at rest and in transit.
> - **Regularly audit** bucket policies and permissions using IAM tools and S3 access logs.


## Amazon S3: Static Website Hosting
Amazon S3 can serve as a simple and scalable web host for **static websites**, delivering HTML, CSS, JavaScript, and other client-side assets without a backend server.

### Website URL Format
The endpoint for accessing a static site depends on the AWS region: 
```
http://bucket-name.s3-website-<aws-region>.amazonaws.com > http://demo-bucket.s3-website-us-west-2.amazonaws.com
http://bucket-name.s3-website.<aws-region>.amazonaws.com > http://demo-bucket.s3-website.us-west-2.amazonaws.com
```

### Configuration Steps
1. **Create a bucket** with the same name as your domain or desired subdomain (e.g., `example.com`).
2. **Upload your static site files** (e.g., `index.html`, `error.html`).
3. Enable **Static Website Hosting** in the bucket properties:
   - Set the index document (e.g., `index.html`)
   - Optionally set an error document (e.g., `error.html`)
4. **Update bucket policy** to allow public reads, if needed (see below).
5. Access the site using the provided S3 website endpoint.

### Troubleshooting
- If you see a **403 Forbidden error**, ensure the **bucket policy** allows `s3:GetObject` access for `Principal: "*"` (i.e., public read access).
- Verify that **Block Public Access** settings are disabled for the bucket.

> üìå *Amazon S3 offers a cost-effective, serverless solution for hosting static websites globally.*


