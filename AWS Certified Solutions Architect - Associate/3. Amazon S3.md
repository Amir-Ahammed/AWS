# Amazon S3 (Simple Storage Service)
Amazon S3 (Simple Storage Service) is a **highly scalable, durable, and secure object storage service** offered by AWS. Itâ€™s designed to store and retrieve **any amount of data** from **anywhere on the web**, making it a backbone for cloud-native applications, data lakes, backups, and more

## Features of Amazon S3

## 1. Storage Classes
- **S3 Standard**: General-purpose, frequent access.
- **S3 Intelligent-Tiering**: Automated cost optimization.
- **S3 Standard-IA / One Zone-IA**: Infrequent access options.
- **S3 Glacier / Glacier Deep Archive**: Archival storage.
- **S3 Express One Zone**: Ultra-low latency, single AZ.

### Storage Classes with Pricing (Mumbai Region)

| Storage Class              | Retrieval Time     | Min. Duration | Use Case                          | Approx. Storage Price/GB*        |
|---------------------------|--------------------|---------------|-----------------------------------|----------------------------------|
| S3 Standard               | Milliseconds       | None          | Active content, high demand       | $0.023                           |
| S3 Intelligent-Tiering    | Millisec to Hours  | 30â€“180 days   | Unpredictable access patterns     | $0.023 (frequent), + monitoring fee |
| S3 Standard-IA            | Milliseconds       | 30 days       | Backups, infrequent access        | $0.016                           |
| S3 One Zone-IA            | Milliseconds       | 30 days       | Low-priority, single-AZ storage   | $0.0128                          |
| S3 Glacier Instant        | Milliseconds       | 90 days       | Archives with instant access      | $0.0115                          |
| S3 Glacier Flexible       | 1 min â€“ 12 hours   | 90 days       | Long-term archives, flexible needs| $0.0046                          |
| S3 Glacier Deep Archive   | 12 â€“ 48 hours      | 180 days      | Long-term, lowest-cost storage    | $0.00099                         |


## 2. Storage Management
- **S3 Lifecycle**: Automate storage tier transitions and object expiration to optimize cost and retention.
- **S3 Object Lock**: Prevent object deletion or modification for a set time to meet compliance (WORM).
- **S3 Replication**: Automatically copy objects between buckets (same or different regions) for backup, compliance, or low-latency access.
- **S3 Batch Operations**: Perform actions (like copy, tag, restore, or invoke Lambda) on billions of objects in bulk with a single job.


## S3/Lifecycle
An S3 Lifecycle Policy is a rule that automates what happens to your objects over timeâ€”like moving them to cheaper storage or deleting them when they're no longer needed.

### Why use it?
- To **save money** by moving infrequently accessed data to lower-cost storage classes.
- To **automate cleanup** by deleting old or temporary files you no longer need.
- To maintain **data compliance**, ensuring files are retained or deleted on a schedule that meets legal or business policies.
- To manage **object versioning**, automatically removing old versions that accumulate over time.
- To clean up **incomplete multipart uploads** and avoid being charged for unfinished file transfers.
- To support an **automated archival strategy**, like sending inactive documents or media files to Glacier without writing custom code.

### How It Works
- You can **define** rules inside a bucket.
- Each rule can target **specific objects** (using prefixes or tags).
- You set **conditions** like:
  - "After X days, move object to cheaper storage,â€ or â€œDelete object after Y days.â€ (Ex: After 30 days â†’ move to S3 Glacier, After 365 days â†’ delete)
  - AWS watches the object age and applies the action automatically.

> ðŸ“Œ Tip: Lifecycle rules apply to both **existing** and **new** objects that match the rule criteria.

## S3/Object Lock
S3 Object Lock is a feature that lets you **protect objects from being deleted or overwritten** for a fixed time or indefinitely, using a **WORM (Write Once, Read Many)** model.

### Why use it?
- To meet **compliance requirements** (e.g., SEC, FINRA) that require data immutability.
- To protect critical data from **accidental or malicious deletion** (e.g., ransomware).
- To enforce **retention policies** for legal or business needs.

### How It Work?
1. **Enable versioning** on the bucket (required).
2. Turn on **Object Lock** when creating the bucket.
3. Apply either:
   - A **Retention Period**: Locks the object for a set time.
   - A **Legal Hold**: Locks the object until manually removed.
4. During the lock, the object **cannot be modified or deleted**, even by admins.

> ðŸ“Œ Tip: Use S3 Object Lock with versioning to safeguard critical data from accidental or malicious deletionâ€”even administrators canâ€™t bypass it during the retention period.

## S3/Replication
S3 Replication automatically copies objects from one bucket to anotherâ€”either in the same AWS Region or across different Regions.

### Why use it?
- **Disaster recovery**: Keep backups in separate locations.
- **Compliance**: Meet data residency or redundancy requirements.
- **Low-latency access**: Serve users faster by storing data closer to them.
- **Cross-account backup**: Replicate to buckets in other AWS accounts.

### Types of Replication
**There are two types of replication:**
- **Live replication â€“ To automatically replicate new and updated objects** as they are written to the source bucket, use live replication. Live replication doesn't replicate any objects that existed in the bucket before you set up replication. To replicate objects that existed before you set up replication, use on-demand replication.
  - **There are two forms of live replication:**
    - **Cross-Region Replication (CRR)** â€“ You can use CRR to replicate objects across Amazon S3 buckets in different AWS Regions.
    - **Same-Region Replication (SRR)** â€“ You can use SRR to copy objects across Amazon S3 buckets in the same AWS Region
- **On-demand replication â€“ To replicate existing objects** from the source bucket to one or more destination buckets on demand, use **S3 Batch Replication**.
- **Additional Replications**
  - **Multi-Destination Replication**: Replicates from one source bucket to *multiple* destination buckets.
  - **S3 Replication Time Control (RTC)**: Guarantees that 99.99% of new objects are replicated within 15 minutes.
  - **bi-directional replication**: objects are automatically copied in both directions between two S3 bucketsâ€”so each bucket stays in sync with the other.

### How does it work?
1. Enable **versioning** on both source and destination buckets.
2. Create a **replication rule** in the source bucket.
3. Define:
   - Destination bucket
   - Filter (prefix or tag)
   - Storage class (optional)
   - Replication options (e.g., delete marker replication, metrics)

***Example Replication Rule (JSON)***
```json
{
  "Role": "arn:aws:iam::123456789012:role/s3-replication-role",
  "Rules": [
    {
      "ID": "replicate-logs",
      "Status": "Enabled",
      "Prefix": "logs/",
      "Destination": {
        "Bucket": "arn:aws:s3:::my-destination-bucket",
        "StorageClass": "STANDARD_IA"
      }
    }
  ]
}
```
## 3. Security & Access Control
Amazon S3 offers a combination of ***user-based*** and ***resource-based*** security mechanisms to control access to ***buckets*** and ***objects***. Security in S3 is managed primarily through ***IAM***, ***Bucket policies***, ***Access Control Lists,*** and ***Encryption***.

### 1. User-Based Access Control
* **IAM Policies**
  - Define which **API actions** a specific IAM **user, group, or role** can perform.
  - Managed centrally in **IAM**, not in the S3 service directly.
  - Can allow or deny operations like `s3:GetObject`, `s3:PutObject`, etc.

***Example: User Access to S3 â€” Using IAM Permission*** <br> **Use Case:** Grant specific IAM users or groups access to S3 buckets using IAM policies.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::examplebucket",
        "arn:aws:s3:::examplebucket/*"
      ]
    }
  ]
}
```
> ðŸ“Œ IAM policies are user-based and should follow the principle of least privilege.

***Example: EC2 Instance Access â€” Using IAM Role*** <br> **Use Case:** Allow an EC2 instance to access an S3 bucket (e.g., for data processing or logging).
**Steps:**
- Create an **IAM Role** with an attached policy for S3 access.
- Attach the role to your EC2 instance.
- The EC2 instance uses **temporary credentials** to access S3.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```
> ðŸ“Œ Ideal for avoiding long-term access keys in instance profiles.

### 2. Resource-Based Access Control
* **Bucket Policy (most common)** 
  - JSON-based policy attached to a bucket.
  - Can grant **cross-account access**.
  - Useful for **public access settings**, **service permissions**, and **fine-grained control** over bucket access.
* **Object Access Control List (ACL)**
  - Legacy method for granting **individual permissions** to specific users for individual objects.
  - Can be disabled for security best practice.
* **Bucket Access Control List (ACL)**
  - Legacy method for setting permissions at the bucket level.
  - Rarely used today; can also be disabled.

### Bucket Policies
A **bucket policy** is a JSON-based resource policy attached directly to an S3 bucket. It defines **who can access** the bucket or its objects, **which actions** are allowed or denied, and **what conditions** apply to those permissions.

### Key Elements of a Bucket Policy
- **Effect**: Allow or Deny.
- **Principal**: The user, role, or account the policy applies to.
- **Action**: Specific S3 API actions (e.g., `s3:GetObject`, `s3:PutObject`).
- **Resource**: Amazon Resource Name (ARN)l of the bucket or objects (e.g., `arn:aws:s3:::examplebucket/*`).
- **Condition** (optional): Constraints like IP address, secure transport, or MFA.

***Example: Public Access â€” Using Bucket Policy*** <br> **Use Case:** Allow anyone on the internet to read objects in a bucket (common for hosting static websites or public datasets).

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```
> ðŸ“Œ Important: Always check and block public access settings intentionally if making a bucket public.

***Example: Cross-Account Access â€” Using Bucket Policy*** <br> **Use Case:** Allow another AWS account to access your S3 bucket.
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "CrossAccountAccess",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:root"
      },
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```
> ðŸ“Œ This setup allows users from Account B to access resources in Account A's S3 bucket.

### Encryption
When you store data in S3, Amazon can encrypt it to keep it secure
- S3 supports two types of Protection/Encryption:
  - **at rest:** When data is stored (saved).
  - **in transit:** When data is being sent over the internet.
- Encryption Options:
  - **SSE-S3 (Server-Side Encryption with S3-managed keys)**: Server-side encryption with Amazon-managed keys.
  - **SSE-KMS (Server-Side Encryption with AWS Key Management Service)**: Encryption with AWS KMS-managed keys.
  - **SSE-C (Server-Side Encryption with Customer-provided keys)**: Customer-provided keys.
- Clients can also encrypt data **before uploading** to S3.

### Block Public Access
AWS provides settings to block public access to S3 buckets at the account or bucket level. These settings help prevent accidental exposure of sensitive data.

### Configuration Table

| Setting                                                                 | Default | Description                                                                 |
|-------------------------------------------------------------------------|---------|-----------------------------------------------------------------------------|
| Block all public access                                                 | On      | Disables all public access regardless of ACLs or policies                   |
| Block public access through new ACLs                                    | On      | Prevents public access granted via newly created ACLs                       |
| Block public access through any ACLs                                    | On      | Denies all public access granted via any ACLs                               |
| Block public access through new public bucket/access point policies     | On      | Blocks new attempts to make the bucket public using policies                |
| Block public and cross-account access via any bucket/access point policies | On   | Disallows both public and cross-account access unless explicitly permitted  |

> ðŸ“Œ These settings can be configured at the **account level** or **per bucket**.  
> ðŸ“Œ Recommended for most use cases where buckets are not intended to be public.

### Authorization Logic
An IAM principal (user or role) can access an S3 object **if:**
- The **IAM policy allows** it **OR** the **resource policy (e.g., bucket policy)** allows it  
**AND**
- There is **no explicit deny** in any applicable policy


## 4. Data Processing & Query
- **S3 Select**: SQL-like query on object content.
- **Amazon Athena**: Serverless query for S3 data.
- **S3 Object Lambda**: Customize object retrieval dynamically.

## 5. Monitoring & Analytics
- **S3 Storage Lens**: Usage metrics across accounts.
- **Access Logs**: Track request activity.
- **CloudWatch Integration**: Operational monitoring.

## 7. Scalability & Durability
- 99.999999999% durability (11 9s).
- Virtually unlimited scalability.
- Multi-part upload for large files.


### Core Concepts
| Term            | Description                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| **Bucket**      | Top-level container for objects (like a folder).                           |
| **Object**      | Data stored in S3, consisting of data, metadata, and a unique key.          |
| **Key**         | Unique identifier for an object within a bucket.                           |
| **Region**      | Physical location where your bucket/data resides.                          |

## Amazon S3: Common Uses
1. **Backup and Storage**
   - Serves as a central repository for data backups.
   - Supports versioning and lifecycle policies for long-term data retention.
2. **Disaster Recovery**
   - Enables quick recovery of critical data in case of failure.
   - Can replicate data across multiple regions for high availability.
3. **Archiving**
   - Offers low-cost storage classes (e.g., Glacier) ideal for infrequently accessed data.
   - Suitable for regulatory or compliance-driven archiving.
4. **Hybrid Cloud Storage**
   - Acts as an extension of on-premises infrastructure.
   - Tools like AWS Storage Gateway allow integration between on-prem and cloud storage.
5. **Application Hosting**
   - Stores static assets (e.g., HTML, CSS, JavaScript) for web or mobile apps.
   - Enables integration with AWS Lambda or EC2 for dynamic applications.
6. **Media Hosting**
   - Hosts videos, audio, and images at scale.
   - Works well with Amazon CloudFront for content delivery.
7. **Data Lakes and Big Data Analytics**
   - Central storage for structured and unstructured data.
   - Integrates with services like Amazon Athena, Redshift, and EMR for data processing.
8. **Software Delivery**
   - Distributes software binaries, patches, updates, and installers globally.
   - Supports signed URLs and access control for secure delivery.
9. **Static Website Hosting**
    - Allows fully hosted, serverless websites.
    - Supports routing rules, custom domains, and error handling.

> ðŸ“Œ *Amazon S3 is foundational to many AWS services and architecturesâ€”mastering it sets the stage for cloud-native development.*

## Amazon S3: Buckets 
An Amazon S3 **bucket** is a container for storing **objects** (data files). All objects are stored in buckets, making them a foundational concept in S3.
- Each object is stored in a specific **bucket**.
- Buckets must have a **globally unique name (across all regions all accounts)**
- Buckets are **created in a specific AWS region** but can be accessed globally (S3 *appears* global).
- Proper bucket naming is critical for DNS compatibility and service integration.

### Buckets: Naming Rules
- Bucket names must be **globally unique** across all AWS accounts and regions.
- Name requirements:
  - Must be **3â€“63 characters** in length.
  - No uppercase letters or underscores.
  - Must start with a **lowercase letter or number**.
  - Cannot be formatted as an IP address (e.g., `192.168.1.1`).
  - Must **not** start with the prefix `xn--`.
  - Must **not** end with the suffix `-s3alias`.

## Amazon S3: Object
In Amazon S3, an **object** is the fundamental unit of storage. Each object contains:
- **Data**: The actual content (e.g., file, image, JSON).
- **Metadata**: Information about the object in key:value form (either system-defined or user-defined).
  - When you upload an image to Amazon S3, metadata provides additional information about the object.
  - System-defined Metadata:
    - `Content-Type: image/jpeg` â€“ Tells the browser it's a JPEG image.
  - User-defined Metadata:
    - `x-amz-meta-title: Beach Sunset` â€“ Custom title set by the uploader.
    - `x-amz-meta-uploaded-by: Amir` â€“ Indicates who uploaded the image.
- **Key**: The unique identifier for the object within a bucket.
  - The `key` is like the full path or filename in the bucket.
  - S3 doesnâ€™t have real foldersâ€”the â€œfolder structureâ€ is simulated by key naming using slashes (`/`).
  - Example of Key: `documents/reports/2024/q1-summary.pdf`
    - Prefix: `documents/reports/2024/`
    - Object Name: `q1-summary.pdf`
- **Version ID**: A unique identifier for the version of the object (if versioning is enabled).
  - When versioning is enabled on a bucket, each object version gets a unique ID.
  - Example: `VersionId: 3HL4kqtJlcpXrof3fjRbJ6G2f8mFvbzF`
- **Tags**: Up to 10 Unicode key:value pairs assigned to help with lifecycle management, security, or classification.
  - Tags in Amazon S3 are key:value pairs that help you organize and manage your objects.
  - Example Tags for an Image File:
    - `Project: Marketing2025`
    - `Owner: Amir`
    - `Environment: Production`

### Object: Limitations
- Objects larger than **5GB** require a **multi-part upload** process.
- Maximum object size: **5TB (5000GB)**

## Amazon S3: Security
Amazon S3 offers a combination of ***user-based*** and ***resource-based*** security mechanisms to control access to ***buckets*** and ***objects***. Security in S3 is managed primarily through ***IAM***, ***Bucket policies***, ***Access Control Lists,*** and ***Encryption***.



> ## Best Practices
> - Use **IAM roles** for applications instead of long-term access keys.
> - **Block public access** at the account and bucket level unless explicitly needed.
> - Enforce **encryption** for both data at rest and in transit.
> - **Regularly audit** bucket policies and permissions using IAM tools and S3 access logs.





## Amazon S3: Static Website Hosting
Amazon S3 can serve as a simple and scalable web host for **static websites**, delivering HTML, CSS, JavaScript, and other client-side assets without a backend server.

### Website URL Format
The endpoint for accessing a static site depends on the AWS region: 
```
http://bucket-name.s3-website-<aws-region>.amazonaws.com > http://demo-bucket.s3-website-us-west-2.amazonaws.com
http://bucket-name.s3-website.<aws-region>.amazonaws.com > http://demo-bucket.s3-website.us-west-2.amazonaws.com
```

### Configuration Steps
1. **Create a bucket** with the same name as your domain or desired subdomain (e.g., `example.com`).
2. **Upload your static site files** (e.g., `index.html`, `error.html`).
3. Enable **Static Website Hosting** in the bucket properties:
   - Set the index document (e.g., `index.html`)
   - Optionally set an error document (e.g., `error.html`)
4. **Update bucket policy** to allow public reads, if needed (see below).
5. Access the site using the provided S3 website endpoint.

### Troubleshooting
- If you see a **403 Forbidden error**, ensure the **bucket policy** allows `s3:GetObject` access for `Principal: "*"` (i.e., public read access).
- Verify that **Block Public Access** settings are disabled for the bucket.

> ðŸ“Œ *Amazon S3 offers a cost-effective, serverless solution for hosting static websites globally.*

## Amazon S3: Versioning
Amazon S3 **versioning** allows you to preserve, retrieve, and restore every version of every object stored in a bucket. It helps protect against accidental overwrites and deletions.

### Key Concepts
- **Enabled at the bucket level**.
- When versioning is enabled:
  - Uploading a file with the **same key** creates a **new version** (e.g., version 1, 2, 3...).
  - You can retrieve or restore previous versions if needed.

> **Best Practice**: Enable versioning to protect important data & revert to earlier versions easily.

### Notes
- Files that existed **before versioning** was enabled are assigned a special version: `"null"`.
- **Suspending versioning** does not delete existing versionsâ€”it only stops new versions from being created.

### Visual Example

A file stored in:  
`s3://my-bucket/my-file.docx`

May have:
- Version 1  
- Version 2  
- Version 3  
...and so on.

Each version is uniquely identified and retrievable.

> ðŸ“Œ *Versioning adds resilience and traceability to your data, making it an essential feature for secure and recoverable storage.*

## Amazon S3: Replication
Amazon S3 Replication enables automatic, asynchronous copying of objects from one S3 bucket to another. It supports compliance, disaster recovery, data locality, and low-latency access use cases.

### Key â€“ Features
- **Asynchronous processing** â€“ Slight delay after uploads
- **Preserves metadata** â€“ Includes tags, version IDs, timestamps
- **Flexible destination classes** â€“ Glacier, Deep Archive, etc.
- **Multiple destination buckets** â€“ From one source
- **Delete Marker replication** â€“ Optional for versioned buckets
- **S3 Replication Time Control (RTC)** â€“ SLA: 99.99% within 15 minutes

### Key â€“ Characteristics & Requirements
- Source and destination buckets can belong to **the same or different AWS accounts**.
- **Versioning must be enabled** on both the source and destination buckets.
- An **IAM role** is required with the following permissions:
  - `s3:ReplicateObject`
  - `s3:ReplicateDelete`
  - `s3:GetObjectVersion`
- **Replication rules** define **what/all objects** objects to replicate (e.g., by prefix or tag).
- The **destination bucket policy** must allow replication from the source account.
- Destination objects are **read-only** by default to prevent tampering.

### Types of Replication
1. **Same-Region Replication (SRR)**
   - Replicates objects between buckets in the **same AWS region**.
   - Useful for:
     - Aggregating logs from multiple buckets.
     - Syncing production and test environments.
2. **Cross-Region Replication (CRR)**
   - Replicates objects between buckets in **different AWS regions**.
   - Useful for:
     - Compliance with data sovereignty rules.
     - Reducing latency by storing data closer to users.
     - Cross-account data backup.
3. **Batch Replication**
   - **By default**, only *new objects* are replicated after enabling replication.
   - **S3 Batch Replication** can be used to:
     - Replicate **existing objects** that predate the replication setup.
     - Retry **previously failed replications**.
3. **Live Replication**
   - Automatically replicates new and updated objects from a source bucket to one or more destination buckets as soon as they are written.
   - Applies to both **Same-Region Replication (SRR)** and **Cross-Region Replication (CRR**).
   - Can be paired with **S3 Replication Time Control (RTC)** for SLA-backed delivery within 15 minutes
   - Use Cases:
     - Real-time backups for critical data
     - Low-latency access in multiple regions
     - Compliance with data residency and durability requirements


### Additional Note
* **Delete Behavior**
  - **Delete markers** can be replicated to the destination bucket (optional).
  - **Versioned deletions** (i.e., deletes with a version ID) are **not replicated** to avoid malicious or accidental permanent deletions.

* **Replication Chaining**
  - Replication **is not transitive**:
    - If Bucket A replicates to Bucket B, and B replicates to C â€” objects uploaded to A **will not reach C**.
    - Replication policies only apply *from the direct source bucket*.

### Example Use Cases

| Use Case                                | Recommended Replication Type |
|-----------------------------------------|------------------------------|
| Compliance (e.g., cross-region storage) | CRR                          |
| Low-latency access in remote regions    | CRR                          |
| Centralized log aggregation             | SRR                          |
| Environment sync (dev/test/prod)        | SRR                          |
| Backup to separate AWS account/region   | CRR or SRR                   |

> ðŸ“Œ *Amazon S3 Replication increases durability, availability, and compliance flexibility by duplicating objects across buckets and regions.*

## Amazon S3: Storage Classes
Amazon S3 offers several storage classes designed to help balance **cost**, **durability**, **availability**, and **access frequency**. Choosing the right class depends on how frequently you access your data and your performance requirements.

### S3 Standard
- Purpose: Frequently accessed, performance-critical data.
- Availability: 99.99%
- Durability: 99.999999999% (11 9s)
- Cost: Highest among S3 tiers
- Use Case: Web/mobile app assets, big data analytics, dynamic workloads

### S3 Intelligent-Tiering
- Purpose: Automatically moves objects between tiers based on access frequency
- Tiers: Frequent, Infrequent, Archive Instant, Deep Archive
- Monitoring Fee: Small monthly charge per object
- No retrieval fees (in frequent tiers)
- Use Case: Unknown or changing access patterns

### S3 Standard-IA (Infrequent Access)
- Purpose: Infrequently accessed but needed quickly
- Retrieval Time: Milliseconds
- Minimum Storage Duration: 30 days
- Lower storage cost, but retrieval cost applies
- Use Case: Long-term backups, disaster recovery

### S3 One Zone-IA
- Purpose: Same as Standard-IA but stored in only one Availability Zone
- Availability: Lower due to single AZ storage
- Cost: Cheaper than Standard-IA
- Use Case: Re-creatable, non-critical data (e.g., thumbnails, temp files)

### S3 Glacier Instant Retrieval
- Retrieval Time: Milliseconds
- Minimum Storage Duration: 90 days
- Durability: Same as Standard
- Cost: Lower than IA, higher than deep Glacier
- Use Case: Archive data with periodic access (e.g., compliance files, medical records)

### S3 Glacier Flexible Retrieval  
*Formerly â€œAmazon S3 Glacierâ€*
- Retrieval Times:
  - Expedited: 1â€“5 minutes
  - Standard: 3â€“5 hours
  - Bulk: 5â€“12 hours (often free)
- Minimum Storage Duration: 90 days
- Use Case: Archive data not needed urgently (e.g., regulatory data, logs)

### S3 Glacier Deep Archive
- Retrieval Times:
  - Standard: 12 hours
  - Bulk: up to 48 hours
- Minimum Storage Duration: 180 days
- Lowest cost option
- Use Case: Long-term archival (e.g., legal records, raw footage)

### Amazon S3 Storage Classes with Pricing (Mumbai Region)

| Storage Class              | Retrieval Time     | Min. Duration | Use Case                          | Approx. Storage Price/GB*        |
|---------------------------|--------------------|---------------|-----------------------------------|----------------------------------|
| S3 Standard               | Milliseconds       | None          | Active content, high demand       | $0.023                           |
| S3 Intelligent-Tiering    | Millisec to Hours  | 30â€“180 days   | Unpredictable access patterns     | $0.023 (frequent), + monitoring fee |
| S3 Standard-IA            | Milliseconds       | 30 days       | Backups, infrequent access        | $0.016                           |
| S3 One Zone-IA            | Milliseconds       | 30 days       | Low-priority, single-AZ storage   | $0.0128                          |
| S3 Glacier Instant        | Milliseconds       | 90 days       | Archives with instant access      | $0.0115                          |
| S3 Glacier Flexible       | 1 min â€“ 12 hours   | 90 days       | Long-term archives, flexible needs| $0.0046                          |
| S3 Glacier Deep Archive   | 12 â€“ 48 hours      | 180 days      | Long-term, lowest-cost storage    | $0.00099                         |

> ðŸ“ *Pricing is approximate and may vary. Always confirm with the [AWS Pricing Calculator](https://calculator.aws.amazon.com/) for the latest rates.*
> ðŸ“Œ *Using the right S3 storage class ensures that youâ€™re paying for what you needâ€”nothing more, nothing lessâ€”while still retaining AWSâ€™s hallmark scalability and durability.*



## Amazon S3: Best Practices

| Category              | Best Practice                                                                 |
|-----------------------|-------------------------------------------------------------------------------|
| **Naming**            | Use globally unique, clear, and consistent bucket names                       |
| **Security**          | Block public access, use IAM roles/policies, and enable encryption (SSE)      |
| **Access Control**    | Disable ACLs; use Object Ownership and bucket policies                        |
| **Versioning**        | Enable versioning to protect against accidental deletes/overwrites            |
| **Lifecycle**         | Set lifecycle rules to transition or delete old/unused objects                |
| **Tagging**           | Apply tags for cost tracking, automation, and organization                    |
| **Performance**       | Use prefixes (folders) for logical grouping and better performance            |
| **Cost Optimization** | Choose the right storage class (e.g., Standard, Glacier)                      |
| **Monitoring**        | Enable CloudTrail, S3 access logs, and CloudWatch for auditing and alerts     |
| **Data Integrity**    | Use checksums (ETag or ChecksumAlgorithm) to verify object integrity          |
| **Replication**       | Use CRR or SRR for backup and cross-region redundancy                         |
